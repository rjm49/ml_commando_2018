{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M/L Commando Course, Cambridge 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Our usual scenario for learning tasks include a list of instances (represented as feature/value pairs) and a special feature (the target class) that we want to predict for future instances based on the values of the remaining features. However, the source data does not usually come in this format. We have to extract what we think are potentially useful features and convert them to our learning format. This process is called feature extraction or feature engineering, and it is an often underestimated but very important and time-consuming phase in most real- world machine learning tasks._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing numpy, scikit-learn, pandas, and pyplot, the Python libraries we will be using in this chapter. Show the versions we will be using (in case you have problems running the notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print('IPython version:', IPython.__version__)\n",
    "print('numpy version:', np.__version__)\n",
    "print('scikit-learn version:', sk.__version__)\n",
    "print('matplotlib version:', matplotlib.__version__)\n",
    "print('pandas version:', pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import titanic data using pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python package pandas (http://pandas.pydata.org/), for example, provides data structures and tools for data analysis. It aims to provide similar features to those of R, the popular language and environment for statistical computing. We will use pandas to import the Titanic data we presented in Chapter 2, Supervised Learning, and convert them to the scikit-learn format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_raw = pd.read_csv('data/titanic.csv')\n",
    "print (titanic_raw[12:14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that each csv column has a corresponding feature into the DataFrame, and that the feature type is induced from the available data. We can inspect some features to see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (titanic_raw.head(14)[['pclass', 'survived', 'age', 'embarked', 'boat', 'sex']]) \n",
    "# \"head\" method just gets us the first n rows\n",
    "#<-rjm49 note mix of indexing and list notation [[]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_raw.describe() #rjm49 - handy Pandas summarisation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, scikit-learn methods expect real numbers\n",
    "as feature values.  Last time, we used the LabelEncoder and OneHotEncoder preprocessing methods to manually convert certain categorical features into 1-of-K values (generating a new feature for each possible value; valued 1 if the original feature had the corresponding value and 0 otherwise). This time, we will use a similar scikit-learn method, DictVectorizer, which automatically builds these features from the different original feature values. Moreover, we will program a method to encode a set of columns in a unique step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction\n",
    "\n",
    "def one_hot_dataframe(data, cols, replace=False):\n",
    "    \"\"\" Takes a dataframe and a list of columns that need to be encoded.\n",
    "    Returns a 3-tuple comprising the data, the vectorized data,\n",
    "    and the fitted vectorizor.\n",
    "    Modified from https://gist.github.com/kljensen/5452382\n",
    "    \"\"\"\n",
    "    dic_vecr = feature_extraction.DictVectorizer()\n",
    "    stuff_to_transform = data[cols]\n",
    "    print(stuff_to_transform.head(),\"\\n\")\n",
    "    dty_list = stuff_to_transform.to_dict(orient='records') #rjm49 - this call makes a dict for each record in the table, returns them all as a list\n",
    "    print(\"first dict:\", dty_list[0:14],\"\\n\") #rjm49 - look at the first one to see what's in it...\n",
    "    \n",
    "    txd = dic_vecr.fit_transform(dty_list) #converts string types to 1-hot-encoded classes as a NumPy \"sparse array\"\n",
    "    print(\"first transformed vec:\\n\", txd[0],\"\\n\")\n",
    "\n",
    "    vecData = pd.DataFrame( txd.toarray())\n",
    "    vecData.columns = dic_vecr.get_feature_names()\n",
    "    vecData.index = data.index\n",
    "    if replace is True: #replace the columns in data with those from our VecData object\n",
    "        data = data.drop(cols, axis=1)\n",
    "        data = data.join(vecData)\n",
    "    return (data, vecData)\n",
    "\n",
    "titanic, titanic_n= one_hot_dataframe(titanic_raw, ['pclass', 'embarked', 'sex'], replace=True)\n",
    "print(titanic.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heck is going on with the \"embarked\" feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (titanic_n['embarked'])\n",
    "print (\"- - - -\")\n",
    "mask = (titanic_n['embarked'] != 0) #returns a boolean True/False Series\n",
    "print (titanic_n[mask].head()['embarked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the remaining categorical features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic, titanic_n = one_hot_dataframe(titanic, ['home.dest', 'room', 'ticket', 'boat'], replace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to deal with missing values, since DecisionTreeClassifier we plan to use does not admit them on input. Pandas allow us to replace them with a fixed value using the fillna method. We will use the mean age for the age feature, and 0 for the remaining missing attributes. Adjust N/A ages with the mean age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (titanic['age'].describe())\n",
    "print (titanic[12:14]['age'])\n",
    "\n",
    "print(\"- - - - -\")\n",
    "mean = titanic['age'].mean()\n",
    "titanic['age'].fillna(mean, inplace=True)\n",
    "print (titanic['age'].describe())\n",
    "print (titanic[12:14]['age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete n/a with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (titanic.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "titanic_target = titanic['survived']\n",
    "titanic_data = titanic.drop(['name', 'row.names', 'survived'], axis=1) #can use inplace=True to alter original\n",
    "X_train, X_test, y_train, y_test = train_test_split(titanic_data, titanic_target, test_size=0.25, random_state=33)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how a decision tree works with the current feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "dt = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "dt = dt.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot, io\n",
    "dot_data = io.StringIO()\n",
    "sk.tree.export_graphviz(dt, out_file=dot_data, feature_names=titanic_data.columns)\n",
    "#graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "(graph,) = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "graph.write_png('titanic.png')\n",
    "from IPython.core.display import Image\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def measure_performance(X, y, clf, show_accuracy=True, show_classification_report=True, show_confusion_matrix=True):\n",
    "    y_pred = clf.predict(X)   \n",
    "    if show_accuracy:\n",
    "         print( \"Accuracy:{0:.3f}\".format(metrics.accuracy_score(y, y_pred)),\"\\n\")\n",
    "    if show_classification_report:\n",
    "        print (\"Classification report\")\n",
    "        print (metrics.classification_report(y, y_pred),\"\\n\")\n",
    "      \n",
    "    if show_confusion_matrix:\n",
    "        print (\"Confusion matrix\")\n",
    "        print (metrics.confusion_matrix(y, y_pred),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "measure_performance(X_test, y_test, dt, show_confusion_matrix=False, show_classification_report=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with a smaller feature set may lead to better results. So we want to find some way to algorithmically find the best features. This task is called feature selection and is a crucial step when we aim to get decent results with machine learning algorithms. If we have poor features, our algorithm will return poor results no matter how sophisticated our machine learning algorithm is. Select only the 20% most important features, using a chi2 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_selection\n",
    "fs = feature_selection.SelectPercentile(feature_selection.chi2, percentile=20)\n",
    "X_train_fs = fs.fit_transform(X_train, y_train)\n",
    "print (titanic_data.columns[fs.get_support()])\n",
    "print (fs.scores_[2])\n",
    "print (titanic_data.columns[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate performance with the new feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(X_train_fs, y_train)\n",
    "X_test_fs = fs.transform(X_test)\n",
    "measure_performance(X_test_fs, y_test, dt, show_confusion_matrix=False, show_classification_report=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best percentil using cross-validation on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection #cross_validation\n",
    "\n",
    "percentiles = range(1, 100, 5)\n",
    "results = []\n",
    "for i in percentiles:\n",
    "    fs = feature_selection.SelectPercentile(feature_selection.chi2, percentile=i)\n",
    "    X_train_fs = fs.fit_transform(X_train, y_train)\n",
    "    scores = cross_validation.cross_val_score(dt, X_train_fs, y_train, cv=5)\n",
    "    #print i,scores.mean()\n",
    "    results = np.append(results, scores.mean())\n",
    "\n",
    "optimal_percentil = np.where(results == results.max())[0]\n",
    "print(optimal_percentil)\n",
    "print(percentiles)\n",
    "#print (percentiles[optimal_percentil[0]])\n",
    "print (\"Optimal number of features:\", percentiles[optimal_percentil[0]], \"\\n\")\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "import pylab as pl\n",
    "pl.figure()\n",
    "pl.xlabel(\"Number of features selected\")\n",
    "pl.ylabel(\"Cross validation accuracy)\")\n",
    "pl.plot(percentiles,results)\n",
    "print (\"Mean scores:\",results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate our best number of features on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = feature_selection.SelectPercentile(feature_selection.chi2, percentile=percentiles[optimal_percentil[0]])\n",
    "X_train_fs = fs.fit_transform(X_train, y_train) #rjm49 - select just the most relevant features, train on those\n",
    "dt.fit(X_train_fs, y_train)\n",
    "X_test_fs = fs.transform(X_test)\n",
    "measure_performance(X_test_fs, y_test, dt, show_confusion_matrix=False, show_classification_report=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dt.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the best criterion, using the held out set (see next notebook on Model Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "scores = cross_validation.cross_val_score(dt, X_train_fs, y_train, cv=5)\n",
    "print( \"Entropy criterion accuracy on cv: {0:.3f}\".format(scores.mean()))\n",
    "dt = tree.DecisionTreeClassifier(criterion='gini')\n",
    "scores = cross_validation.cross_val_score(dt, X_train_fs, y_train, cv=5)\n",
    "print( \"Gini criterion accuracy on cv: {0:.3f}\".format(scores.mean()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(X_train_fs, y_train)\n",
    "X_test_fs = fs.transform(X_test)\n",
    "measure_performance(X_test_fs, y_test, dt, show_confusion_matrix=False, show_classification_report=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
