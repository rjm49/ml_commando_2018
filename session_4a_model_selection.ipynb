{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M/L Commando Course, Cambridge 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In the previous section we worked on ways to preprocess the data and select the most promising features. As we stated, selecting a good set of features is a crucial step to obtain good results. Now we will focus on another important step: selecting the algorithm parameters, known as hyperparameters to distinguish them from the parameters that are adjusted within the machine learning algorithm. Many machine learning algorithms include hyperparameters (from now on we will simply call them parameters) that guide certain aspects of the underlying method and have great impact on the results. In this section we will review some methods to help us obtain the best parameter configuration, a process known as model selection.\n",
    "We will look back at the text-classification problem we addressed in Chapter 2, Supervised Learning. In that example, we compounded a TF-IDF vectorizer alongside a multinomial Naïve Bayes (NB) algorithm to classify a set of newsgroup messages into a discrete number of categories. The MultinomialNB algorithm has one important parameter, named alpha, that adjusts the smoothing. We initially used the class with its default parameter values (alpha = 1.0) and obtained an accuracy of 0.89. But when we set alpha to 0.01, we obtained a noticeable accuracy improvement to 0.92. Clearly, the configuration of the alpha parameter has great impact on the performance of the algorithm. How can we be sure 0.01 is the best value? Perhaps if we try other possible values, we could still obtain better results._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing numpy, scikit-learn, and pyplot, the Python libraries we will be using in this chapter. Show the versions we will be using (in case you have problems running the notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import IPython\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print ('IPython version:', IPython.__version__)\n",
    "print ('numpy version:', np.__version__)\n",
    "print ('scikit-learn version:', sk.__version__)\n",
    "print ('matplotlib version:', matplotlib.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start again with our text-classification problem, but for now we will only use a reduced number of instances. We will work only with 3,000 instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news = fetch_20newsgroups(subset='all')\n",
    "\n",
    "n_samples = 3000\n",
    "\n",
    "X = news.data[:n_samples]\n",
    "y = news.target[:n_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import the set of stop words and create a pipeline that compounds the TF-IDF vectorizer and the Naïve Bayes algorithms (recall that we had a stopwords_en.txt file with a list of stop words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_stop_words():\n",
    "    result = set()\n",
    "    for line in open('data/stopwords_en.txt', 'r').readlines():\n",
    "        result.add(line.strip())\n",
    "    return result\n",
    "\n",
    "stop_words = get_stop_words()\n",
    "print(random.sample(stop_words,50))\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                stop_words=stop_words,\n",
    "                token_pattern=r\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",         \n",
    "    )),\n",
    "    ('nb', MultinomialNB(alpha=0.01)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we evaluate our algorithm with a three-fold cross-validation, we obtain a mean score of around 0.81.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from scipy.stats import sem\n",
    "\n",
    "def evaluate_cross_validation(clf, X, y, K):\n",
    "    # create a k-fold croos validation iterator of k=5 folds\n",
    "    cv = KFold(len(y), K, shuffle=True, random_state=0)\n",
    "    # by default the score used is the one returned by score method of the estimator (accuracy)\n",
    "    scores = cross_val_score(clf, X, y, cv=cv)\n",
    "    print (scores)\n",
    "    print ((\"Mean score: {0:.3f} (+/-{1:.3f})\").format(\n",
    "        np.mean(scores), sem(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_cross_validation(clf, X, y, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we should train the algorithm with a list of different parameter values and keep the parameter value that achieves the best results. Let's implement a helper function to do that. This function will train the algorithm with a list of values, each time obtaining an accuracy score calculated by performing k-fold cross-validation\n",
    "on the training instances. After that, it will plot the training and testing scores as a function of the parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_params(X, y, clf, param_values, param_name, K):\n",
    "    # initialize training and testing scores with zeros\n",
    "    train_scores = np.zeros(len(param_values))\n",
    "    test_scores = np.zeros(len(param_values))\n",
    "    \n",
    "    # iterate over the different parameter values\n",
    "    for i, param_value in enumerate(param_values):\n",
    "        print (param_name, ' = ', param_value)\n",
    "        \n",
    "        # set classifier parameters\n",
    "        clf.set_params(**{param_name:param_value}) #rjm49 - double-splat convention converts dict to keyword args e.g. clf.set_params(param_name=param_value)\n",
    "        \n",
    "        # initialize the K scores obtained for each fold\n",
    "        k_train_scores = np.zeros(K)\n",
    "        k_test_scores = np.zeros(K)\n",
    "        \n",
    "        # create KFold cross validation\n",
    "        cv = KFold(n_samples, K, shuffle=True, random_state=0)\n",
    "        \n",
    "        # iterate over the K folds\n",
    "        for j, (train, test) in enumerate(cv):\n",
    "            # fit the classifier in the corresponding fold\n",
    "            # and obtain the corresponding accuracy scores on train and test sets\n",
    "            clf.fit([X[k] for k in train], y[train])\n",
    "            k_train_scores[j] = clf.score([X[k] for k in train], y[train])\n",
    "            k_test_scores[j] = clf.score([X[k] for k in test], y[test])\n",
    "            \n",
    "        # store the mean of the K fold scores\n",
    "        train_scores[i] = np.mean(k_train_scores)\n",
    "        test_scores[i] = np.mean(k_test_scores)\n",
    "       \n",
    "    # plot the training and testing scores in a log scale\n",
    "    plt.semilogx(param_values, train_scores, alpha=0.4, lw=2, c='b')\n",
    "    plt.semilogx(param_values, test_scores, alpha=0.4, lw=2, c='g')\n",
    "    plt.legend([\"train\",\"test\"])\n",
    "    plt.xlabel(param_name + \" values\")\n",
    "    plt.ylabel(\"Mean cross validation accuracy\")\n",
    "\n",
    "    # return the training and testing scores on each parameter value\n",
    "    return train_scores, test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call this function; we will use numpy's logspace function to generate a list of alpha values spaced evenly on a log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-7, 0, 8)\n",
    "print (alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores, test_scores = calc_params(X, y, clf, alphas, 'nb__alpha', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the training accuracy is always greater than the testing accuracy. The best results are obtained with an alpha value of 0.1 (accuracy of 0.81):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('training scores: ', train_scores)\n",
    "print ('testing scores: ', test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a very useful function to graph and obtain the best parameter value for a classifier. Let's use it to adjust another classifier that uses a Support Vector Machines (SVM) instead of MultinomialNB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                stop_words=stop_words,\n",
    "                token_pattern=r\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",         \n",
    "    )),\n",
    "    ('svc', SVC()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = np.logspace(-2, 1, 4)\n",
    "\n",
    "train_scores, test_scores = calc_params(X, y, clf, gammas, 'svc__gamma', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('training scores: ', train_scores)\n",
    "print ('testing scores: ', test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For gamma < 1 we have underfitting. For gamma > 1 we have overfitting. So here, the best result is for gamma = 1 where we obtain a training an accuracy of 0.999 and a testing accuracy of about 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take a closer look at the SVC class constructor parameters, we have other parameters, apart from gamma, that may also affect classifier performance. If we only adjust the gamma value, we implicitly state that the optimal C value is 1.0 (the default value that we did not explicitly set). Perhaps we could obtain better results with a new combination of C and gamma values. This opens a new degree of complexity; we should try all the parameter combinations and keep the better one.\n",
    "\n",
    "With GridSearchCV, we can specify a grid of any number of parameters and parameter values to traverse. It will train the classifier for each combination and obtain a cross-validation accuracy to evaluate each one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'svc__gamma': np.logspace(-2, 1, 4),\n",
    "    'svc__C': np.logspace(-1, 1, 3),\n",
    "}\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                stop_words=stop_words,\n",
    "                token_pattern=r\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",         \n",
    "    )),\n",
    "    ('svc', SVC()),\n",
    "])\n",
    "\n",
    "gs = GridSearchCV(clf, parameters, verbose=2, refit=False, cv=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute our grid search and print the best parameter values and scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = gs.fit(X, y) # rjm49 - IPython magic function to record the timestamp when this was called\n",
    "#gs.fit(X, y)\n",
    "\n",
    "gs.best_params_, gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the grid search we obtained a better combination of C and gamma parameters, for values 10.0 and 0.10 respectively, we obtained a 3-fold cross validation accuracy of 0.828 much better than the best value we obtained (0.76) in the previous experiment by only adjusting gamma and keeeping C value at 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could continue trying to improve the results by also adjusting the vectorizer parameters in the grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search calculation grows exponentially with each parameter and its possible values we want to tune. We could reduce our response time if we calculate each of the combinations in parallel instead of sequentially, as we have done. In our previous example, we had four different values for gamma and three different values for C, summing up 12 parameter combinations. Additionally, we also needed to train each combination three times (in a three-fold cross-validation), so we summed up\n",
    "36 trainings and evaluations. We could try to run these 36 tasks in parallel, since the tasks are independent.\n",
    "\n",
    "Most modern computers have multiple cores that can be used to run tasks in parallel. We also have a very useful tool within IPython, a standalone module called ipyparallel (formerly IPython parallel), that allows us to run independent tasks in parallel, each task in a different core of our machine. Let's do that with our text classifier example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will declare a function that will persist all the K folds for the cross validation in different files. These files will be loaded by a process that will execute the corresponding fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "import os\n",
    "\n",
    "def persist_cv_splits(X, y, K=3, name='data', suffix=\"_cv_%03d.pkl\"):\n",
    "    \"\"\"Dump K folds to filesystem.\"\"\"\n",
    "    \n",
    "    cv_split_filenames = []\n",
    "    \n",
    "    # create KFold cross validation\n",
    "    cv = KFold(n_samples, K, shuffle=True, random_state=0)\n",
    "    \n",
    "    # iterate over the K folds\n",
    "    for i, (train, test) in enumerate(cv):\n",
    "        cv_fold = ([X[k] for k in train], y[train], [X[k] for k in test], y[test])\n",
    "        cv_split_filename = name + suffix % i\n",
    "        cv_split_filename = os.path.abspath(cv_split_filename)\n",
    "        joblib.dump(cv_fold, cv_split_filename)\n",
    "        cv_split_filenames.append(cv_split_filename)\n",
    "    \n",
    "    return cv_split_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_filenames = persist_cv_splits(X, y, name='news')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function loads a particular fold and fits the classifier with the specified parameters set. Finally returns the testing score. This function will be called by each of the parallel processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_evaluation(cv_split_filename, clf, params):\n",
    "    \n",
    "    # All module imports should be executed in the worker namespace\n",
    "    from sklearn.externals import joblib\n",
    "\n",
    "    # load the fold training and testing partitions from the filesystem\n",
    "    X_train, y_train, X_test, y_test = joblib.load(\n",
    "        cv_split_filename, mmap_mode='c')\n",
    "    \n",
    "    clf.set_params(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    test_score = clf.score(X_test, y_test)\n",
    "    return test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function executes the grid search in parallel processes. For each of the parameter combination (returned by the IterGrid iterator), it iterates over the K folds and creates a process to compute the evaluation. It returns the parameter combinations alongside with the tasks list: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import ParameterGrid\n",
    "\n",
    "def parallel_grid_search(lb_view, clf, cv_split_filenames, param_grid):\n",
    "    \n",
    "    all_tasks = []\n",
    "    all_parameters = list(ParameterGrid(param_grid))\n",
    "    \n",
    "    # iterate over parameter combinations\n",
    "    for i, params in enumerate(all_parameters):\n",
    "        task_for_params = []\n",
    "        \n",
    "        # iterate over the K folds\n",
    "        for j, cv_split_filename in enumerate(cv_split_filenames):    \n",
    "            t = lb_view.apply(\n",
    "                compute_evaluation, cv_split_filename, clf, params)\n",
    "            task_for_params.append(t) \n",
    "        \n",
    "        all_tasks.append(task_for_params)\n",
    "        \n",
    "    return all_parameters, all_tasks"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we use IPython parallel to get the client and a load balanced view. We must first create a local cluster of N engines by using the Cluster tab in the IPython notebook. Then we create the client, the view and execute our parallel_grid_search function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from IPython.parallel import Client\n",
    "\n",
    "client = Client()\n",
    "lb_view = client.load_balanced_view()\n",
    "\n",
    "all_parameters, all_tasks = parallel_grid_search(\n",
    "   lb_view, clf, cv_filenames, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress(tasks):\n",
    "    progress = np.mean([task.ready() for task_group in tasks\n",
    "                                 for task in task_group])\n",
    "    print (\"Tasks completed: {0}%\".format(100 * progress) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_progress(all_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bests(all_parameters, all_tasks, n_top=5):\n",
    "    \"\"\"Compute the mean score of the completed tasks\"\"\"\n",
    "    mean_scores = []\n",
    "    \n",
    "    for param, task_group in zip(all_parameters, all_tasks):\n",
    "        scores = [t.get() for t in task_group if t.ready()]\n",
    "        if len(scores) == 0:\n",
    "            continue\n",
    "        mean_scores.append((np.mean(scores), param))\n",
    "                   \n",
    "    return sorted(mean_scores, reverse=True)[:n_top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (find_bests(all_parameters, all_tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
