{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Commando Course, Cambridge 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 2a Text Classification with Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook might not work properly without:\n",
    "jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000\n",
    "\n",
    "_One of the most successful applications of Naïve Bayes has been within the field\n",
    "of Natural Language Processing (NLP). NLP is a field that has been much related\n",
    "to machine learning, since many of its problems can be formulated as a classification task. Usually, NLP problems have important amounts of tagged data in the form of text documents. This data can be used as a training dataset for machine\n",
    "learning algorithms.\n",
    "In this section, we will use Naïve Bayes for text classification; we will have a set of text documents with their corresponding categories, and we will train a Naïve Bayes algorithm to learn to predict the categories of new unseen instances. This simple task has many practical applications; probably the most known and widely used one is spam filtering. In this section we will try to classify newsgroup messages using a dataset that can be retrieved from within scikit-learn. This dataset consists of around 19,000 newsgroup messages from 20 different topics ranging from politics and religion to sports and science_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing numpy, scikit-learn, and pyplot, the Python libraries we will be using in this chapter. Show the versions we will be using (in case you have problems running the notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "IPython version: 6.2.1\n",
      "numpy version: 1.13.3\n",
      "scikit-learn version: 0.19.1\n",
      "matplotlib version: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import IPython\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "print ('IPython version:', IPython.__version__)\n",
    "print ('numpy version:', np.__version__)\n",
    "print ('scikit-learn version:', sk.__version__)\n",
    "print ('matplotlib version:', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the newsgroup Dataset, and explore its structure and data (this could take some time, especially if sklearn has to download the 14MB dataset from the Internet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news = fetch_20newsgroups(subset='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the dataset structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR', 'description'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(news))\n",
    "news.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the properties of the dataset, we will find that we have the usual ones: DESCR, data, target, and target_names. The difference now is that data holds a list of text contents, instead of a numpy matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'numpy.ndarray'> <class 'list'>\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "18846\n",
      "18846\n"
     ]
    }
   ],
   "source": [
    "print (type(news.data), type(news.target), type(news.target_names))\n",
    "print (news.target_names)\n",
    "print (len(news.data))\n",
    "print (len(news.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at a random instance, you will see the content of a newsgroup message, and you can get its corresponding category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: gt6511a@prism.gatech.EDU (COCHRANE,JAMES SHAPLEIGH)\n",
      "Subject: Re: Change of name ??\n",
      "Organization: Georgia Institute of Technology\n",
      "Lines: 35\n",
      "\n",
      "In article <CMM.0.90.2.735315429.thomasp@holmenkollen.ifi.uio.no> thomasp@ifi.uio.no (Thomas Parsli) writes:\n",
      ":\n",
      ":\n",
      ":\t1. Make a new Newsgroup called talk.politics.guns.PARANOID or \n",
      ":\ttalk.politics.guns.THEY'R.HERE.TO.TAKE.ME.AWAY\n",
      ":\n",
      ":\t2. Move all postings about waco and burn to (guess where)..\n",
      ":\n",
      ":\t3. Stop posting #### on this newsgroup\n",
      ";\n",
      ":\tWe are all SO glad you're trying to save us from the evil \n",
      ":\tgoverment, but would you mail this #### in regular mail to\n",
      ":\tlet's say 1000 people ????\n",
      ":\t\n",
      ":\n",
      ":                        Thomas Parsli\n",
      "And everybody who talked about the evil arising in Europe was labeled \n",
      "reactionary in the late 1930's... after all, we could negotiate with Hitler and\n",
      "trust him to keep his end of the bargain... at least that's what Stalin and\n",
      "Chamberlin thought... I guess they forgot to teach you about your country being\n",
      "overrun by the Germans in WWII, 'eh Thomas?  And I'm sorry you consider outrage\n",
      "at government excesses to be ####... Everytime the Israelis conduct a mass \n",
      "operation against a terrorist group that is actively killing their citizens and\n",
      "soldiers, the world gets indignant, but it's ok for the US to assault it's own\n",
      "citizens who were a religous minority and accused of sexual deviation and \n",
      "hoarding weapons... I find it real ironic this happened the same day Al Gore\n",
      "arrived in Poland to recognize the sacrifices made in the Warsaw Ghetto where\n",
      "the same 'justifications' were raised for an armed assault by black-clad troops\n",
      "with armor support...  \n",
      "\n",
      "-- \n",
      "********************************************************************************\n",
      "James S. Cochrane        *  When in danger, or in doubt, run in * This space \n",
      "gt6511a@prism.gatech.edu *  circles, scream and shout.          * for rent\n",
      "********************************************************************************\n",
      "\n",
      "16 talk.politics.guns\n"
     ]
    }
   ],
   "source": [
    "rand_ix = np.random.randint(0, len(news.data))\n",
    "print (news.data[rand_ix])\n",
    "category = news.target[rand_ix]\n",
    "category_name = news.target_names[category]\n",
    "print (category, category_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the training and testing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rjm49 - this time we're doing a manual split\n",
    "SPLIT_PERC = 0.75\n",
    "split_size = int(len(news.data)*SPLIT_PERC)\n",
    "X_train = news.data[:split_size]\n",
    "X_test = news.data[split_size:]\n",
    "y_train = news.target[:split_size]\n",
    "y_test = news.target[split_size:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will serve to perform and evaluate a cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from scipy.stats import sem\n",
    "\n",
    "def evaluate_cross_validation(clf, X, y, K):\n",
    "    # create a k-fold croos validation iterator of k=5 folds\n",
    "    print(\"underway...\")\n",
    "    k_cv = KFold(n_splits=K, shuffle=True, random_state=0)\n",
    "    # by default the score used is the one returned by score method of the estimator (accuracy)\n",
    "    scores = cross_val_score(clf, X, y, cv=k_cv)\n",
    "    print (scores)\n",
    "    print ((\"Mean score: {0:.3f} (+/-{1:.3f})\").format(\n",
    "        np.mean(scores), sem(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our machine learning algorithms can work only on numeric data...\n",
    "\n",
    "Inside the `sklearn.feature_extraction.text` module, there are three classes that can transform text into numeric features: `CountVectorizer`, `HashingVectorizer`, and `TfidfVectorizer`.\n",
    "The difference between them resides in the calculations they perform to derive the numeric features:\n",
    "- `CountVectorizer` basically creates a dictionary of words from the corpus. Then, each instance is converted to a vector of numeric features where each element will be the frequency of each word in the document.\n",
    "- `HashingVectorizer`, instead of constructing and maintaining the dictionary in memory, implements a hashing function that maps tokens into feature indexes, and then computes the count as in CountVectorizer. (Sadly seems intentionally broken at the time of writing!)\n",
    "- `TfidfVectorizer` works like CountVectorizer, with a more advanced calculation called \"Term Frequency - Inverse Document Frequency\" (TF-IDF). This is a statistic for measuring the importance of a word in a document or corpus. Intuitively, it looks for words that are more frequent in the current document, compared with their frequency across all documents. You can see this as a way to normalize the results and avoid words that are too frequent, and thus not useful to characterise the instances.\n",
    "\n",
    "We will create a Naïve Bayes classifier that is composed of a feature vectorizer and the actual Bayes classifier. We will use the MultinomialNB class from the `sklearn.naive_bayes` module. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "clf_1 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "# clf_2 = Pipeline([\n",
    "#     ('vect', HashingVectorizer()),\n",
    "#     ('clf', MultinomialNB()),\n",
    "# ])\n",
    "clf_3 = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for clf Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])\n",
      "underway...\n",
      "[ 0.85782493  0.85725657  0.84664367  0.85911382  0.8458477 ]\n",
      "Mean score: 0.853 (+/-0.003)\n",
      "for clf Pipeline(memory=None,\n",
      "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "  ...rue,\n",
      "        vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])\n",
      "underway...\n",
      "[ 0.84482759  0.85990979  0.84558238  0.85990979  0.84213319]\n",
      "Mean score: 0.850 (+/-0.004)\n"
     ]
    }
   ],
   "source": [
    "clfs = [clf_1, clf_3]\n",
    "for clf in clfs:\n",
    "    print(\"for clf {}\".format(clf))\n",
    "    evaluate_cross_validation(clf, news.data, news.target, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will keep the TF-IDF vectorizer but use a different regular expression to perform tokenization. The default regular expression: \"\\w\\w+\" considers alphanumeric characters and the underscore. Perhaps also considering the slash and the dot could improve the tokenization, and begin considering tokens as Wi-Fi and site.com. The new regular expression could be: \"[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\". If you have queries about how to define regular expressions, please refer to the Python re module documentation. Let's try our new classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_4 = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                token_pattern=\"[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\",\n",
    "    )),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=10000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.85702918  0.87476784  0.85752189  0.87052269  0.85805253]\n",
      "Mean score: 0.864 (+/-0.004)\n"
     ]
    }
   ],
   "source": [
    "print(news.data)\n",
    "\n",
    "\n",
    "evaluate_cross_validation(clf_4, news.data, news.target, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another parameter that we can use is stop_words: this argument allows us to pass a list of words we do not want to take into account, such as too frequent words, or words we do not a priori expect to provide information about the particular topic. Let's try to improve performance filtering the stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "def get_stop_words():\n",
    "    return stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = get_stop_words()\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_5 = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                stop_words = stop_words,\n",
    "                token_pattern='[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+',    \n",
    "    )),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "underway...\n",
      "[ 0.87612732  0.8896259   0.87689042  0.88803396  0.87848236]\n",
      "Mean score: 0.882 (+/-0.003)\n"
     ]
    }
   ],
   "source": [
    "evaluate_cross_validation(clf_5, news.data, news.target, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to improve the classification.  Change the max number of features, the smoothing (alpha) parameter on the MultinomialNB classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_7 = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                max_features=None,\n",
    "                stop_words=stop_words,\n",
    "                token_pattern=\"[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\",         \n",
    "    )),\n",
    "    ('clf', MultinomialNB(alpha=0.1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "underway...\n",
      "[ 0.91246684  0.91430088  0.90793314  0.91536217  0.91164765]\n",
      "Mean score: 0.912 (+/-0.001)\n"
     ]
    }
   ],
   "source": [
    "evaluate_cross_validation(clf_7, news.data, news.target, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we could continue doing trials by using different values of alpha or doing new modifications of the vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we decide that we have made enough improvements in our model, we are ready to evaluate its performance on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def train_and_evaluate(clf, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print (\"Accuracy on training set:\")\n",
    "    print (clf.score(X_train, y_train))\n",
    "    print (\"Accuracy on testing set:\")\n",
    "    print (clf.score(X_test, y_test))\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print (\"Classification Report:\")\n",
    "    print (metrics.classification_report(y_test, y_pred))\n",
    "    print (\"Confusion Matrix:\")\n",
    "    print (metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n",
      "0.988113768218\n",
      "Accuracy on testing set:\n",
      "0.90662139219\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.87      0.89       216\n",
      "          1       0.86      0.83      0.84       246\n",
      "          2       0.91      0.84      0.87       274\n",
      "          3       0.78      0.88      0.83       235\n",
      "          4       0.90      0.90      0.90       231\n",
      "          5       0.87      0.92      0.89       225\n",
      "          6       0.88      0.76      0.81       248\n",
      "          7       0.93      0.92      0.92       275\n",
      "          8       0.95      0.97      0.96       226\n",
      "          9       0.97      0.97      0.97       250\n",
      "         10       0.98      1.00      0.99       257\n",
      "         11       0.94      0.98      0.96       261\n",
      "         12       0.88      0.90      0.89       216\n",
      "         13       0.95      0.95      0.95       257\n",
      "         14       0.94      0.96      0.95       246\n",
      "         15       0.81      0.96      0.88       234\n",
      "         16       0.85      0.98      0.91       218\n",
      "         17       0.96      1.00      0.98       236\n",
      "         18       0.96      0.84      0.90       213\n",
      "         19       0.93      0.58      0.72       148\n",
      "\n",
      "avg / total       0.91      0.91      0.91      4712\n",
      "\n",
      "Confusion Matrix:\n",
      "[[188   0   0   0   1   0   0   1   0   0   0   0   0   1   0  18   2   0\n",
      "    0   5]\n",
      " [  0 203   6   5   2  15   3   0   0   0   0   4   3   1   2   0   0   2\n",
      "    0   0]\n",
      " [  0   9 229  24   1   6   2   0   1   0   0   0   0   0   1   0   1   0\n",
      "    0   0]\n",
      " [  1   4   4 207   7   4   3   0   0   0   0   0   3   0   2   0   0   0\n",
      "    0   0]\n",
      " [  0   1   2   6 209   1   5   0   0   0   1   0   5   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   7   4   2   1 206   0   0   1   1   0   0   0   1   1   0   0   1\n",
      "    0   0]\n",
      " [  0   1   3  13   5   0 188  15   2   3   1   3   8   1   2   1   2   0\n",
      "    0   0]\n",
      " [  0   2   1   1   1   0   6 253   4   1   0   0   2   1   0   0   2   0\n",
      "    1   0]\n",
      " [  0   0   0   1   0   1   2   1 219   0   0   1   0   0   0   0   1   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   1   0   1 242   3   0   0   1   0   1   1   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 257   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0   0   0   1   0   0   0   0   0 257   0   1   0   0   1   0\n",
      "    0   0]\n",
      " [  0   3   0   3   5   0   1   1   0   1   1   2 195   1   3   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   1   1   0   0   0   1   0   0   1   2 244   3   2   2   0\n",
      "    0   0]\n",
      " [  0   4   0   0   0   0   0   1   0   0   0   1   1   1 236   0   1   0\n",
      "    1   0]\n",
      " [  1   0   1   2   0   1   1   1   0   0   0   1   0   1   0 225   0   0\n",
      "    0   0]\n",
      " [  0   0   1   0   0   0   1   0   1   0   0   1   0   0   0   0 214   0\n",
      "    0   0]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 235\n",
      "    0   0]\n",
      " [  1   0   0   0   0   0   1   0   1   0   0   2   1   2   1   2  16   6\n",
      "  179   1]\n",
      " [ 14   0   0   0   0   1   0   0   0   1   0   0   1   2   0  29   8   1\n",
      "    5  86]]\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(clf_7, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we obtained very good results, and as we would expect, the accuracy in the training set is quite better than in the testing set. We may expect, in new unseen instances, an accuracy of around 0.91.\n",
    "\n",
    "If we look inside the vectorizer, we can see which tokens have been used to create our dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--------------------------------------------------------------------------------cal',\n",
       " '--------------------------------------------------------------------------------tom',\n",
       " '-------------------------------------------------------------------w--u--w---',\n",
       " '---------------------------------------------------------------tim',\n",
       " '---------------------------------------------ooo--',\n",
       " '---------------------------------signature---------------------------------',\n",
       " '--------------------------------ooo--u--ooo-----------------------',\n",
       " '-------------------------------ching',\n",
       " '-------------------------------cut',\n",
       " '-------------------------------it',\n",
       " '----------------------------ooo--',\n",
       " '----------------------------original',\n",
       " '----------------------------response',\n",
       " '------------------------e-mail',\n",
       " '------------------------the',\n",
       " '-----------------------s-o-u-l---i-s---t-h-e---r-i-d-e-r--------',\n",
       " '---------------------pan',\n",
       " '--------------------ooo-',\n",
       " '--------------------p-------------q---------------------------',\n",
       " '-------------------cut',\n",
       " '----------------cut',\n",
       " '---------------benjamin',\n",
       " '------------jackatak.raider.net',\n",
       " '-----------ooo--',\n",
       " '-----------some',\n",
       " '----------end',\n",
       " '----------everybody',\n",
       " '----------i70----------....---------',\n",
       " '----------mike.',\n",
       " '----------time',\n",
       " '---------pcs',\n",
       " '---------visit',\n",
       " '--------h.',\n",
       " '------10k--------',\n",
       " '------australian-national-university--',\n",
       " '------michael-smith-------------------',\n",
       " '------open----------',\n",
       " '------tac',\n",
       " '-----b-o-d-y---i-s---t-h-e---b-i-k-e----------------------------',\n",
       " '-----begin',\n",
       " '-----cut',\n",
       " '-----end',\n",
       " '-----hitler',\n",
       " '-----je',\n",
       " '-----john',\n",
       " '-----or',\n",
       " '-----working-----',\n",
       " '----bi',\n",
       " '----chris',\n",
       " '----cleveland',\n",
       " '----cut',\n",
       " '----email',\n",
       " '----excuse',\n",
       " '----go----',\n",
       " '----jguastel',\n",
       " '----jon',\n",
       " '----like',\n",
       " '----nextmail',\n",
       " '----robert',\n",
       " '----sample-------------------------------------',\n",
       " '----the',\n",
       " '---also',\n",
       " '---author---',\n",
       " '---barrak',\n",
       " '---benjamin',\n",
       " '---bill',\n",
       " '---build---',\n",
       " '---but',\n",
       " '---compiled',\n",
       " '---cut',\n",
       " '---dale',\n",
       " '---dan',\n",
       " '---data---',\n",
       " '---edward',\n",
       " '---end',\n",
       " '---end---',\n",
       " '---entry---',\n",
       " '---faster',\n",
       " '---from',\n",
       " '---hey',\n",
       " '---i.e.',\n",
       " '---info---',\n",
       " '---it',\n",
       " '---joel',\n",
       " '---john',\n",
       " '---jre---',\n",
       " '---ktj',\n",
       " '---lowell',\n",
       " '---malcusco',\n",
       " '---mark',\n",
       " '---o---',\n",
       " '---or',\n",
       " '---peter',\n",
       " '---piston',\n",
       " '---program---',\n",
       " '---raj',\n",
       " '---ralph',\n",
       " '---remark---',\n",
       " '---rik',\n",
       " '---san',\n",
       " '---sharx--',\n",
       " '---stretches',\n",
       " '---tasmin',\n",
       " '---the',\n",
       " '---then',\n",
       " '---this',\n",
       " '---vinayak',\n",
       " '--a4',\n",
       " '--aamir',\n",
       " '--aaron',\n",
       " '--abhijit',\n",
       " '--actually',\n",
       " '--adam',\n",
       " '--add',\n",
       " '--after',\n",
       " '--al',\n",
       " '--alan',\n",
       " '--albert',\n",
       " '--alex',\n",
       " '--all',\n",
       " '--amos',\n",
       " '--an',\n",
       " '--anania',\n",
       " '--and',\n",
       " '--andre',\n",
       " '--andreas',\n",
       " '--andrew',\n",
       " '--andy',\n",
       " '--anonymous',\n",
       " '--apb',\n",
       " '--aravind',\n",
       " '--are',\n",
       " '--aristophanes',\n",
       " '--as',\n",
       " '--associated',\n",
       " '--atf',\n",
       " '--autoweek',\n",
       " '--b.',\n",
       " '--b.w.',\n",
       " '--babtized',\n",
       " '--barak',\n",
       " '--barbara',\n",
       " '--bartholomaeus',\n",
       " '--bas.',\n",
       " '--berry',\n",
       " '--bill',\n",
       " '--blame',\n",
       " '--bob',\n",
       " '--br',\n",
       " '--brendan',\n",
       " '--brian',\n",
       " '--bruce',\n",
       " '--bukowski',\n",
       " '--bush',\n",
       " '--but',\n",
       " '--by',\n",
       " '--can',\n",
       " '--casey',\n",
       " '--ccm',\n",
       " '--cd',\n",
       " '--ceci',\n",
       " '--ceo',\n",
       " '--check',\n",
       " '--chris',\n",
       " '--chuan.',\n",
       " '--chuck',\n",
       " '--cindy',\n",
       " '--clator',\n",
       " '--clh',\n",
       " '--cliff',\n",
       " '--clinton',\n",
       " '--col',\n",
       " '--constructing',\n",
       " '--cornelius.',\n",
       " '--could',\n",
       " '--craig',\n",
       " '--creation',\n",
       " '--crew',\n",
       " '--curtis',\n",
       " '--dale',\n",
       " '--dan',\n",
       " '--dave',\n",
       " '--dave--',\n",
       " '--david',\n",
       " '--dianne',\n",
       " '--direct',\n",
       " '--display',\n",
       " '--dkm',\n",
       " '--dobermans',\n",
       " '--does',\n",
       " '--doesn',\n",
       " '--don',\n",
       " '--dong',\n",
       " '--donnie',\n",
       " '--dov',\n",
       " '--dr.',\n",
       " '--drywid',\n",
       " '--duke',\n",
       " '--duncan',\n",
       " '--dy2',\n",
       " '--easier',\n",
       " '--easy',\n",
       " '--ed',\n",
       " '--ed.',\n",
       " '--eesh.',\n",
       " '--eric',\n",
       " '--erich',\n",
       " '--even',\n",
       " '--evolution',\n",
       " '--f.',\n",
       " '--fax',\n",
       " '--four',\n",
       " '--fourth',\n",
       " '--francis',\n",
       " '--frank',\n",
       " '--fred',\n",
       " '--fritzm.',\n",
       " '--from',\n",
       " '--fully',\n",
       " '--g-a',\n",
       " '--gandalf',\n",
       " '--gary',\n",
       " '--garym',\n",
       " '--gavin',\n",
       " '--gen.',\n",
       " '--gene',\n",
       " '--george',\n",
       " '--gidi',\n",
       " '--god',\n",
       " '--good',\n",
       " '--greg',\n",
       " '--groucho',\n",
       " '--gun',\n",
       " '--had',\n",
       " '--hal',\n",
       " '--handgun',\n",
       " '--happy',\n",
       " '--harry',\n",
       " '--have',\n",
       " '--hbj',\n",
       " '--he',\n",
       " '--help',\n",
       " '--him.',\n",
       " '--however',\n",
       " '--hudson',\n",
       " '--hymie',\n",
       " '--i.e.',\n",
       " '--if',\n",
       " '--imitation',\n",
       " '--in',\n",
       " '--including',\n",
       " '--intrusion',\n",
       " '--it',\n",
       " '--james',\n",
       " '--jamie',\n",
       " '--jamshid',\n",
       " '--javed.',\n",
       " '--jay',\n",
       " '--jeff',\n",
       " '--jeremy',\n",
       " '--jh',\n",
       " '--jhb',\n",
       " '--jim',\n",
       " '--jjobermark',\n",
       " '--jmn',\n",
       " '--jody',\n",
       " '--joe',\n",
       " '--john',\n",
       " '--josh',\n",
       " '--jun',\n",
       " '--kasey',\n",
       " '--ken',\n",
       " '--kent',\n",
       " '--kerry',\n",
       " '--kim',\n",
       " '--kinda',\n",
       " '--king',\n",
       " '--kurt',\n",
       " '--kyler',\n",
       " '--lbutler',\n",
       " '--leah',\n",
       " '--lecture',\n",
       " '--lee',\n",
       " '--let',\n",
       " '--lowell',\n",
       " '--maarten',\n",
       " '--mack',\n",
       " '--marc',\n",
       " '--mark',\n",
       " '--marlena',\n",
       " '--martin',\n",
       " '--matt',\n",
       " '--matthew',\n",
       " '--me',\n",
       " '--melissa',\n",
       " '--member',\n",
       " '--michael',\n",
       " '--michele',\n",
       " '--mike',\n",
       " '--misc',\n",
       " '--missed',\n",
       " '--moxie',\n",
       " '--msa',\n",
       " '--myron.',\n",
       " '--n.y.c.l.s.',\n",
       " '--naomi',\n",
       " '--negotiable',\n",
       " '--never',\n",
       " '--new',\n",
       " '--nizam',\n",
       " '--not',\n",
       " '--o--',\n",
       " '--of',\n",
       " '--ooo-------',\n",
       " '--ooo-----------',\n",
       " '--ooo------------------------------------',\n",
       " '--or',\n",
       " '--or--',\n",
       " '--osvaldo',\n",
       " '--ozk',\n",
       " '--p.j.',\n",
       " '--parinoia',\n",
       " '--parms.',\n",
       " '--part',\n",
       " '--patrick.',\n",
       " '--paul',\n",
       " '--peg',\n",
       " '--pete',\n",
       " '--phew',\n",
       " '--philip',\n",
       " '--pinghua',\n",
       " '--pink',\n",
       " '--pjr',\n",
       " '--pjz',\n",
       " '--plh',\n",
       " '--polonius',\n",
       " '--price',\n",
       " '--pv',\n",
       " '--q_',\n",
       " '--ralph',\n",
       " '--raman',\n",
       " '--randy',\n",
       " '--ray',\n",
       " '--raymond',\n",
       " '--recognize',\n",
       " '--regis',\n",
       " '--return',\n",
       " '--reversing',\n",
       " '--rex',\n",
       " '--rich',\n",
       " '--rob',\n",
       " '--rob.',\n",
       " '--rod',\n",
       " '--ron--',\n",
       " '--royal',\n",
       " '--rumors',\n",
       " '--russ',\n",
       " '--salty',\n",
       " '--scott',\n",
       " '--senator',\n",
       " '--shannon',\n",
       " '--so',\n",
       " '--society',\n",
       " '--sonny',\n",
       " '--sort',\n",
       " '--squidonk--',\n",
       " '--st',\n",
       " '--standard',\n",
       " '--steeven',\n",
       " '--steve',\n",
       " '--strychnine',\n",
       " '--subtlety',\n",
       " '--t-a',\n",
       " '--t.s.',\n",
       " '--tact',\n",
       " '--ted',\n",
       " '--text',\n",
       " '--the',\n",
       " '--there',\n",
       " '--this',\n",
       " '--thomas',\n",
       " '--thumbs',\n",
       " '--tim',\n",
       " '--tom',\n",
       " '--tony',\n",
       " '--turn',\n",
       " '--tussman',\n",
       " '--uk',\n",
       " '--uncle',\n",
       " '--unfortunately',\n",
       " '--unix',\n",
       " '--unknown',\n",
       " '--until',\n",
       " '--use',\n",
       " '--uuname',\n",
       " '--v.',\n",
       " '--v.i.',\n",
       " '--very',\n",
       " '--vibration',\n",
       " '--victor',\n",
       " '--video',\n",
       " '--videomaker',\n",
       " '--vishwa',\n",
       " '--waiting',\n",
       " '--warren',\n",
       " '--was',\n",
       " '--webster',\n",
       " '--went',\n",
       " '--wes',\n",
       " '--western',\n",
       " '--what',\n",
       " '--whatever',\n",
       " '--when',\n",
       " '--why',\n",
       " '--will',\n",
       " '--without',\n",
       " '--writes',\n",
       " '--yes',\n",
       " '--you',\n",
       " '--yuan',\n",
       " '--yvan',\n",
       " '-...actually',\n",
       " '-.1ph',\n",
       " '-.1r7',\n",
       " '-.2ty',\n",
       " '-.3ec',\n",
       " '-.bc',\n",
       " '-.bd',\n",
       " '-.bdz',\n",
       " '-.bhi.',\n",
       " '-.bn-',\n",
       " '-.cub',\n",
       " '-.dy',\n",
       " '-.is',\n",
       " '-.lqcyd',\n",
       " '-.or',\n",
       " '-.s976yvkp',\n",
       " '-.t_x-',\n",
       " '-.u3',\n",
       " '-.u3l',\n",
       " '-.u5',\n",
       " '-.u5u',\n",
       " '-.u5x',\n",
       " '-.uiwx.c',\n",
       " '-.uiwx.c0qh',\n",
       " '-.uiwx.c4',\n",
       " '-.uiwx.tt',\n",
       " '-.ul7x.c0rg',\n",
       " '-.vp',\n",
       " '-.vpgiz',\n",
       " '-.wm4',\n",
       " '-.xeblaz01ox5',\n",
       " '-.yia',\n",
       " '-.yub',\n",
       " '-.z1dk',\n",
       " '-0.2d-_',\n",
       " '-0.c4',\n",
       " '-0.tt',\n",
       " '-00lz8bct',\n",
       " '-0600lines',\n",
       " '-0aw',\n",
       " '-0hq4',\n",
       " '-0la1z',\n",
       " '-0lc',\n",
       " '-0qw',\n",
       " '-0v34b',\n",
       " '-0wmfhhm',\n",
       " '-0xb',\n",
       " '-10th',\n",
       " '-1240x1024x16',\n",
       " '-12v.',\n",
       " '-12vdc',\n",
       " '-16-character',\n",
       " '-17ha04',\n",
       " '-1_2tmn',\n",
       " '-1_2tmn2v',\n",
       " '-1_2tn082lk4kp3',\n",
       " '-1kjd0',\n",
       " '-1qo',\n",
       " '-1sete',\n",
       " '-1ua',\n",
       " '-1wi',\n",
       " '-1xl',\n",
       " '-1xu',\n",
       " '-1xu-2qt77tl',\n",
       " '-1xuorplors',\n",
       " '-1xuqww',\n",
       " '-1z_o',\n",
       " '-2.sag',\n",
       " '-20db',\n",
       " '-23yiai0',\n",
       " '-24h12',\n",
       " '-25mhz',\n",
       " '-2bz',\n",
       " '-2czl',\n",
       " '-2czs',\n",
       " '-2czslyn',\n",
       " '-2gmg',\n",
       " '-2i2',\n",
       " '-2i49',\n",
       " '-2jf',\n",
       " '-2kj',\n",
       " '-2nh',\n",
       " '-2p8',\n",
       " '-2pl',\n",
       " '-2pl-1xl',\n",
       " '-2pl-2s',\n",
       " '-2pl-34u',\n",
       " '-2pl-7t',\n",
       " '-2plos6_',\n",
       " '-2plq',\n",
       " '-2plqs4l',\n",
       " '-2plqs4l-',\n",
       " '-2pu',\n",
       " '-2pu-2r_o',\n",
       " '-2pu-34lq',\n",
       " '-2puqs4l',\n",
       " '-2puqs7',\n",
       " '-2qi',\n",
       " '-2qt',\n",
       " '-2qt71xl',\n",
       " '-2r_',\n",
       " '-2r_qs4l',\n",
       " '-2rnb9rlahflb',\n",
       " '-2rz',\n",
       " '-2ws',\n",
       " '-320x240',\n",
       " '-34jj',\n",
       " '-34l-6eiq',\n",
       " '-34lorpuq',\n",
       " '-34lq',\n",
       " '-34lqvei',\n",
       " '-34mwh',\n",
       " '-34t2',\n",
       " '-34u-',\n",
       " '-34u-0',\n",
       " '-34u-1n',\n",
       " '-34u-2znl',\n",
       " '-34u-3',\n",
       " '-34u-34',\n",
       " '-34u-34tm',\n",
       " '-34u-34u',\n",
       " '-34u-34u-34',\n",
       " '-34u-34u-34t2l5klk',\n",
       " '-34u-34u-34u-34u-3',\n",
       " '-34u-34u-34u-3l',\n",
       " '-34u-34u-3l',\n",
       " '-34u-34w.smkt-u',\n",
       " '-34u.p4u-3',\n",
       " '-34u.pl',\n",
       " '-34uors',\n",
       " '-34uos4l',\n",
       " '-34uq',\n",
       " '-34uqqxu',\n",
       " '-34uqrplq',\n",
       " '-34uqrs',\n",
       " '-34wdy',\n",
       " '-34we',\n",
       " '-34we36yn',\n",
       " '-357kzu',\n",
       " '-35i-6eiq',\n",
       " '-35i-9',\n",
       " '-35id',\n",
       " '-36_-1xu',\n",
       " '-36_qrs',\n",
       " '-36_qs6',\n",
       " '-36s.',\n",
       " '-386amd',\n",
       " '-386and',\n",
       " '-3celn',\n",
       " '-3db',\n",
       " '-3g.9k',\n",
       " '-3i2d',\n",
       " '-3jc0rv6eh.7prv',\n",
       " '-3qh1',\n",
       " '-3r9n',\n",
       " '-4-bc',\n",
       " '-45m.eh',\n",
       " '-461-5001x431',\n",
       " '-46kmk',\n",
       " '-486dx2-66',\n",
       " '-4bi',\n",
       " '-4fabedo',\n",
       " '-4ghn',\n",
       " '-4ir69a',\n",
       " '-4k-u.s9',\n",
       " '-4mk',\n",
       " '-4n-',\n",
       " '-4te1',\n",
       " '-4veq',\n",
       " '-4zt',\n",
       " '-50yi0',\n",
       " '-51rjpd660-',\n",
       " '-528k-nl',\n",
       " '-5_iyzhl',\n",
       " '-5h08',\n",
       " '-5igj',\n",
       " '-5jq',\n",
       " '-5nf',\n",
       " '-5pm',\n",
       " '-5q253',\n",
       " '-5tl',\n",
       " '-5tu77tl',\n",
       " '-5ut',\n",
       " '-64of',\n",
       " '-6bkp5c',\n",
       " '-6bv',\n",
       " '-6du',\n",
       " '-6duqrs',\n",
       " '-6ei',\n",
       " '-6eiovg',\n",
       " '-6eiqvdud',\n",
       " '-6eiqvf_',\n",
       " '-6eqp',\n",
       " '-6f_',\n",
       " '-6f_-',\n",
       " '-6n_',\n",
       " '-6s7b7',\n",
       " '-6v7',\n",
       " '-70l77ut',\n",
       " '-70u-1xuos4l',\n",
       " '-70u75u',\n",
       " '-71zg',\n",
       " '-7f1xp',\n",
       " '-7s0',\n",
       " '-7tl',\n",
       " '-7tl-5tl',\n",
       " '-7tl-7v_-',\n",
       " '-7tu',\n",
       " '-7tu-2p',\n",
       " '-7tu770l',\n",
       " '-7tuo',\n",
       " '-7ut',\n",
       " '-7v_',\n",
       " '-7v_71yt',\n",
       " '-7v_qs4u',\n",
       " '-7vq',\n",
       " '-8-tp6a',\n",
       " '-800x600x32768',\n",
       " '-89io',\n",
       " '-8b9ilef',\n",
       " '-8c_',\n",
       " '-8cty.',\n",
       " '-8cub',\n",
       " '-8cucgc_',\n",
       " '-8cucsy_',\n",
       " '-8cx',\n",
       " '-8cx_s',\n",
       " '-8cx_s3ea8c',\n",
       " '-8cx_sc_',\n",
       " '-8f-',\n",
       " '-8mng2',\n",
       " '-8xt',\n",
       " '-8y_-8',\n",
       " '-8yd',\n",
       " '-8yx',\n",
       " '-91h8',\n",
       " '-9z05lp',\n",
       " '-_2y_q8',\n",
       " '-_4hspa8',\n",
       " '-_hyz',\n",
       " '-_i18',\n",
       " '-_l97u',\n",
       " '-_xw',\n",
       " '-_y43z',\n",
       " '-a-',\n",
       " '-a.',\n",
       " '-a0',\n",
       " '-a2091',\n",
       " '-a61mbkxyjw.s',\n",
       " '-a8',\n",
       " '-aa',\n",
       " '-aa-z',\n",
       " '-aaron',\n",
       " '-ab',\n",
       " '-ability',\n",
       " '-about-',\n",
       " '-ac',\n",
       " '-acsddc',\n",
       " '-act',\n",
       " '-active-',\n",
       " '-adam',\n",
       " '-adapted',\n",
       " '-adcom',\n",
       " '-adobe-courier-',\n",
       " '-adobe-courier-bold-r-normal--14-140-75-75-m-90-iso8859-1',\n",
       " '-adobe-helvetica-medium-o-normal--0-0-75-75-p-0-iso8859-1',\n",
       " '-adobe-helvetica-medium-o-normal--14-100-90-90-p-0-iso8859-1',\n",
       " '-adrian',\n",
       " '-af',\n",
       " '-af6kijm',\n",
       " '-ahmed.',\n",
       " '-aik-dscn',\n",
       " '-ak6',\n",
       " '-al',\n",
       " '-alan',\n",
       " '-all',\n",
       " '-allan',\n",
       " '-allen',\n",
       " '-allergy',\n",
       " '-almost',\n",
       " '-almost-',\n",
       " '-although',\n",
       " '-am',\n",
       " '-amar',\n",
       " '-amazing',\n",
       " '-ameg',\n",
       " '-amem',\n",
       " '-amiga',\n",
       " '-amir',\n",
       " '-amruth',\n",
       " '-amv',\n",
       " '-an',\n",
       " '-analog',\n",
       " '-anay',\n",
       " '-and',\n",
       " '-andy',\n",
       " '-anon',\n",
       " '-anon.',\n",
       " '-ans.',\n",
       " '-ansel',\n",
       " '-ansi',\n",
       " '-anwar',\n",
       " '-any',\n",
       " '-any-',\n",
       " '-anyone',\n",
       " '-anyone-',\n",
       " '-anywhere',\n",
       " '-apple',\n",
       " '-aq',\n",
       " '-ar1',\n",
       " '-ar2',\n",
       " '-are',\n",
       " '-are-',\n",
       " '-arlo',\n",
       " '-armenian',\n",
       " '-arthur',\n",
       " '-aryian',\n",
       " '-as',\n",
       " '-aseg-m',\n",
       " '-aselk',\n",
       " '-asking',\n",
       " '-asrg',\n",
       " '-assert',\n",
       " '-asv',\n",
       " '-at',\n",
       " '-attributed',\n",
       " '-aub',\n",
       " '-audio',\n",
       " '-author',\n",
       " '-auto',\n",
       " '-autobahn',\n",
       " '-automatic',\n",
       " '-avery',\n",
       " '-avetik',\n",
       " '-away',\n",
       " '-axl',\n",
       " '-axz',\n",
       " '-ay2',\n",
       " '-az.',\n",
       " '-azmi',\n",
       " '-b1',\n",
       " '-b2',\n",
       " '-b4',\n",
       " '-b50',\n",
       " '-b6',\n",
       " '-b617',\n",
       " '-b6ed5s',\n",
       " '-b6v-c8',\n",
       " '-b76g',\n",
       " '-b8',\n",
       " '-b_zg',\n",
       " '-backlit',\n",
       " '-bailey',\n",
       " '-balance',\n",
       " '-balki',\n",
       " '-baloo',\n",
       " '-banked',\n",
       " '-barry',\n",
       " '-based',\n",
       " '-batf',\n",
       " '-battery',\n",
       " '-bb',\n",
       " '-bdynamic',\n",
       " '-be',\n",
       " '-because',\n",
       " '-beetlejuice',\n",
       " '-behind',\n",
       " '-benito',\n",
       " '-bennett',\n",
       " '-bertil-',\n",
       " '-best',\n",
       " '-best24',\n",
       " '-bestbooks',\n",
       " '-bg',\n",
       " '-bias',\n",
       " '-bil',\n",
       " '-bill',\n",
       " '-billy',\n",
       " '-bitmap',\n",
       " '-bitstream-charter-',\n",
       " '-black',\n",
       " '-blocksmooth',\n",
       " '-bo',\n",
       " '-board',\n",
       " '-bob',\n",
       " '-bob-',\n",
       " '-bobby',\n",
       " '-body',\n",
       " '-bold-r-normal-',\n",
       " '-book',\n",
       " '-boris',\n",
       " '-bought',\n",
       " '-bp',\n",
       " '-brad',\n",
       " '-brando',\n",
       " '-brent',\n",
       " '-brian',\n",
       " '-brkint',\n",
       " '-brtv',\n",
       " '-bruce',\n",
       " '-bryan',\n",
       " '-bsb',\n",
       " '-bstatic',\n",
       " '-btw',\n",
       " '-budgetwise-',\n",
       " '-built',\n",
       " '-busch',\n",
       " '-but',\n",
       " '-bw78',\n",
       " '-bx',\n",
       " '-c-',\n",
       " '-c1ill',\n",
       " '-c2f',\n",
       " '-c2t',\n",
       " '-c5',\n",
       " '-c8',\n",
       " '-c845',\n",
       " '-c84hm2',\n",
       " '-c8eg',\n",
       " '-c8rlk8',\n",
       " '-c8v',\n",
       " '-c8v-7u_hz',\n",
       " '-c8v-c8u',\n",
       " '-c8v-c8vhe',\n",
       " '-c8v-fyn',\n",
       " '-c8v-z.c',\n",
       " '-c9',\n",
       " '-c9n',\n",
       " '-c9nd',\n",
       " '-c_ht',\n",
       " '-cable',\n",
       " '-cadence',\n",
       " '-calculator',\n",
       " '-calvin',\n",
       " '-can',\n",
       " '-can-',\n",
       " '-cannot-',\n",
       " '-capture-',\n",
       " '-cards',\n",
       " '-case',\n",
       " '-cated',\n",
       " '-cates.',\n",
       " '-cating',\n",
       " '-cc',\n",
       " '-ccd',\n",
       " '-cd',\n",
       " '-cd-rom',\n",
       " '-certainly',\n",
       " '-cf4-',\n",
       " '-chan.',\n",
       " '-charles',\n",
       " '-checkfree',\n",
       " '-chert',\n",
       " '-cholesterol',\n",
       " '-chris',\n",
       " '-christopher',\n",
       " '-chuck',\n",
       " '-chuck-',\n",
       " '-ciao',\n",
       " '-claus',\n",
       " '-clemens',\n",
       " '-cln',\n",
       " '-clocal',\n",
       " '-clock',\n",
       " '-close-',\n",
       " '-club',\n",
       " '-cne.',\n",
       " '-color',\n",
       " '-comment',\n",
       " '-complete',\n",
       " '-complex',\n",
       " '-consultant',\n",
       " '-converted',\n",
       " '-cooper',\n",
       " '-cornell',\n",
       " '-cornerdelay',\n",
       " '-corners',\n",
       " '-cornhead',\n",
       " '-correct',\n",
       " '-cotact',\n",
       " '-could',\n",
       " '-council',\n",
       " '-cq',\n",
       " '-cr',\n",
       " '-craig',\n",
       " '-craig.',\n",
       " '-crt',\n",
       " '-crts',\n",
       " '-csl',\n",
       " '-cstftjc',\n",
       " '-cstopb',\n",
       " '-ct4.8o',\n",
       " '-cts',\n",
       " '-cuz',\n",
       " '-cw',\n",
       " '-cx',\n",
       " '-cyd',\n",
       " '-d-',\n",
       " '-d.',\n",
       " '-d1h',\n",
       " '-d2j',\n",
       " '-d6',\n",
       " '-d8',\n",
       " '-d8ynlk',\n",
       " '-d8yoq',\n",
       " '-d9',\n",
       " '-d_',\n",
       " '-d__stdc__',\n",
       " '-d_svid',\n",
       " '-daixv3',\n",
       " '-dale',\n",
       " '-dan',\n",
       " '-daniel-',\n",
       " '-danny',\n",
       " '-dark',\n",
       " '-date',\n",
       " '-datebook',\n",
       " '-dave',\n",
       " '-davewood',\n",
       " '-david',\n",
       " '-dawson',\n",
       " '-daycm6p9qev2',\n",
       " '-db',\n",
       " '-dcbq',\n",
       " '-dckq',\n",
       " '-dcurdir',\n",
       " '-death',\n",
       " '-deb',\n",
       " '-dempsey',\n",
       " '-dennis',\n",
       " '-department',\n",
       " '-depth',\n",
       " '-derek',\n",
       " '-dev',\n",
       " '-dew.p',\n",
       " '-dfoo',\n",
       " '-dfuncproto',\n",
       " '-dg',\n",
       " '-diana',\n",
       " '-digital',\n",
       " '-directory',\n",
       " '-discard',\n",
       " '-disconnecting',\n",
       " '-dispgamma',\n",
       " '-display',\n",
       " '-dither',\n",
       " '-djf8',\n",
       " '-dk',\n",
       " '-dkgi',\n",
       " '-dle',\n",
       " '-dlint',\n",
       " '-dm',\n",
       " '-dmalloc_0_returns_null',\n",
       " '-dmotifbc',\n",
       " '-dnodisplay',\n",
       " '-do',\n",
       " '-docs',\n",
       " '-doctor',\n",
       " '-does',\n",
       " '-does-',\n",
       " '-don',\n",
       " '-donavan',\n",
       " '-dong',\n",
       " '-door',\n",
       " '-dopenwin_bug',\n",
       " '-dos_has_locale',\n",
       " '-dos_has_mmap',\n",
       " '-doug',\n",
       " '-douglas',\n",
       " '-dpi',\n",
       " '-dr.banzai',\n",
       " '-draft',\n",
       " '-drawing',\n",
       " '-drt',\n",
       " '-dsco',\n",
       " '-dsharedcode',\n",
       " '-dsuit',\n",
       " '-dsunshlib',\n",
       " '-dsvr4',\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_7.named_steps['vect'].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169933\n"
     ]
    }
   ],
   "source": [
    "print(len(clf_7.named_steps['vect'].get_feature_names()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
